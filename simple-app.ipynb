{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37eb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY']=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e32d88d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kumar\\Desktop\\anaco\\envs\\langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f2cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=WebBaseLoader(\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f4a93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'title': 'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nsentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHugging Face\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tModels\\n\\n\\t\\t\\t\\t\\t\\tDatasets\\n\\n\\t\\t\\t\\t\\t\\tSpaces\\n\\n\\n\\n\\t\\t\\tCommunity\\n\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\tDocs\\n\\n\\t\\t\\t\\t\\t\\tEnterprise\\n\\nPricing\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\nLog In\\n\\t\\t\\t\\t\\nSign Up\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\nsentence-transformers\\n\\n/\\nall-MiniLM-L6-v2\\n\\n\\n\\nlike\\n4.38k\\n\\n\\nFollow\\n\\nSentence Transformers\\n3.49k\\n\\n\\n\\n\\nSentence Similarity\\n\\n\\nsentence-transformers\\n\\n\\nPyTorch\\n\\ngoogle-tensorflow\\nTensorFlow\\n\\n\\nRust\\n\\n\\nONNX\\n\\n\\nSafetensors\\n\\n\\nOpenVINO\\n\\n\\nTransformers\\n\\n\\n\\n21 datasets\\n\\n\\n\\nEnglish\\n\\n\\nbert\\n\\n\\nfeature-extraction\\n\\n\\n\\ntext-embeddings-inference\\n\\n\\n\\narxiv:\\n5 papers\\n\\n\\n\\n\\nLicense:\\napache-2.0\\n\\n\\n\\n\\n\\n\\tModel card\\n\\t\\n\\n\\t\\n\\t\\t\\nFiles\\nFiles and versions\\n\\nxet\\n\\n\\n\\n\\tCommunity\\n\\t147\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tDeploy\\n\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tUse this model\\n\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nall-MiniLM-L6-v2\\nUsage (Sentence-Transformers)\\n\\nUsage (HuggingFace Transformers)\\n\\nBackground\\n\\nIntended uses\\n\\nTraining procedure\\nPre-training\\nFine-tuning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tall-MiniLM-L6-v2\\n\\t\\n\\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\\n\\n\\n\\n\\n\\n\\t\\tUsage (Sentence-Transformers)\\n\\t\\n\\nUsing this model becomes easy when you have sentence-transformers installed:\\npip install -U sentence-transformers\\n\\nThen you can use the model like this:\\nfrom sentence_transformers import SentenceTransformer\\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\\n\\nmodel = SentenceTransformer(\\'sentence-transformers/all-MiniLM-L6-v2\\')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\\n\\n\\n\\n\\n\\n\\n\\t\\tUsage (HuggingFace Transformers)\\n\\t\\n\\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\\nfrom transformers import AutoTokenizer, AutoModel\\nimport torch\\nimport torch.nn.functional as F\\n\\n#Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\n\\n\\n# Sentences we want sentence embeddings for\\nsentences = [\\'This is an example sentence\\', \\'Each sentence is converted\\']\\n\\n# Load model from HuggingFace Hub\\ntokenizer = AutoTokenizer.from_pretrained(\\'sentence-transformers/all-MiniLM-L6-v2\\')\\nmodel = AutoModel.from_pretrained(\\'sentence-transformers/all-MiniLM-L6-v2\\')\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=\\'pt\\')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input[\\'attention_mask\\'])\\n\\n# Normalize embeddings\\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\\n\\nprint(\"Sentence embeddings:\")\\nprint(sentence_embeddings)\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tBackground\\n\\t\\n\\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \\ncontrastive learning objective. We used the pretrained nreimers/MiniLM-L6-H384-uncased model and fine-tuned in on a \\n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\\nWe developed this model during the \\nCommunity week using JAX/Flax for NLP & CV, \\norganized by Hugging Face. We developed this model as part of the project:\\nTrain the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\\n\\n\\n\\n\\n\\n\\t\\tIntended uses\\n\\t\\n\\nOur model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \\nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\\nBy default, input text longer than 256 word pieces is truncated.\\n\\n\\n\\n\\n\\n\\t\\tTraining procedure\\n\\t\\n\\n\\n\\n\\n\\n\\n\\t\\tPre-training\\n\\t\\n\\nWe use the pretrained nreimers/MiniLM-L6-H384-uncased model. Please refer to the model card for more detailed information about the pre-training procedure.\\n\\n\\n\\n\\n\\n\\t\\tFine-tuning\\n\\t\\n\\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\\nWe then apply the cross entropy loss by comparing with true pairs.\\n\\n\\n\\n\\n\\n\\t\\tHyper parameters\\n\\t\\n\\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\\na 2e-5 learning rate. The full training script is accessible in this current repository: train_script.py.\\n\\n\\n\\n\\n\\n\\t\\tTraining data\\n\\t\\n\\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\\nWe sampled each dataset given a weighted probability which configuration is detailed in the data_config.json file.\\n\\n\\n\\nDataset\\nPaper\\nNumber of training tuples\\n\\n\\nReddit comments (2015-2018)\\npaper\\n726,484,430\\n\\n\\nS2ORC Citation pairs (Abstracts)\\npaper\\n116,288,806\\n\\n\\nWikiAnswers Duplicate question pairs\\npaper\\n77,427,422\\n\\n\\nPAQ (Question, Answer) pairs\\npaper\\n64,371,441\\n\\n\\nS2ORC Citation pairs (Titles)\\npaper\\n52,603,982\\n\\n\\nS2ORC (Title, Abstract)\\npaper\\n41,769,185\\n\\n\\nStack Exchange (Title, Body) pairs\\n-\\n25,316,456\\n\\n\\nStack Exchange (Title+Body, Answer) pairs\\n-\\n21,396,559\\n\\n\\nStack Exchange (Title, Answer) pairs\\n-\\n21,396,559\\n\\n\\nMS MARCO triplets\\npaper\\n9,144,553\\n\\n\\nGOOAQ: Open Question Answering with Diverse Answer Types\\npaper\\n3,012,496\\n\\n\\nYahoo Answers (Title, Answer)\\npaper\\n1,198,260\\n\\n\\nCode Search\\n-\\n1,151,414\\n\\n\\nCOCO Image captions\\npaper\\n828,395\\n\\n\\nSPECTER citation triplets\\npaper\\n684,100\\n\\n\\nYahoo Answers (Question, Answer)\\npaper\\n681,164\\n\\n\\nYahoo Answers (Title, Question)\\npaper\\n659,896\\n\\n\\nSearchQA\\npaper\\n582,261\\n\\n\\nEli5\\npaper\\n325,475\\n\\n\\nFlickr 30k\\npaper\\n317,695\\n\\n\\nStack Exchange Duplicate questions (titles)\\n\\n304,525\\n\\n\\nAllNLI (SNLI and MultiNLI\\npaper SNLI, paper MultiNLI\\n277,230\\n\\n\\nStack Exchange Duplicate questions (bodies)\\n\\n250,519\\n\\n\\nStack Exchange Duplicate questions (titles+bodies)\\n\\n250,460\\n\\n\\nSentence Compression\\npaper\\n180,000\\n\\n\\nWikihow\\npaper\\n128,542\\n\\n\\nAltlex\\npaper\\n112,696\\n\\n\\nQuora Question Triplets\\n-\\n103,663\\n\\n\\nSimple Wikipedia\\npaper\\n102,225\\n\\n\\nNatural Questions (NQ)\\npaper\\n100,231\\n\\n\\nSQuAD2.0\\npaper\\n87,599\\n\\n\\nTriviaQA\\n-\\n73,346\\n\\n\\nTotal\\n\\n1,170,060,424\\n\\n\\n\\n\\n\\n\\nDownloads last month149,792,991\\n\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSafetensors\\nModel size\\n22.7M params\\nTensor type\\nI64\\n\\t\\t\\t\\t¬∑F32\\n\\t\\t\\t\\t¬∑\\n\\nFiles info\\n\\n\\n\\n\\n\\n\\n\\nInference Providers\\nNEW\\n\\t\\n\\n\\n\\n\\nHF Inference API\\n\\n\\n\\n\\n\\nSentence Similarity\\n\\n\\n\\nExamples\\n\\n\\n\\n\\n\\nSource Sentence\\n\\n\\n\\nSentences to compare to\\n\\n\\n\\n\\n\\nAdd Sentence\\nGenerate\\n\\n\\n\\n\\nView Code Snippets\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tMaximize\\n\\n\\n\\n\\n\\n\\n\\t\\tModel tree for sentence-transformers/all-MiniLM-L6-v2\\n\\n\\n\\n\\nAdapters\\n\\n14 models\\n\\n\\nFinetunes\\n\\n718 models\\n\\n\\nQuantizations\\n\\n58 models\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tDatasets used to train\\n\\t\\t\\t\\t\\t\\tsentence-transformers/all-MiniLM-L6-v2\\n\\nmandarjoshi/trivia_qa\\n\\n\\n\\t\\t\\tViewer\\n\\t\\t\\t‚Ä¢ \\nUpdated\\n\\t\\t\\t\\t\\tJan 5, 2024\\n‚Ä¢ \\n\\n848k\\n‚Ä¢ \\n\\n\\t\\t\\t\\t\\t43k\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t‚Ä¢ \\n\\n\\t\\t\\t\\t\\t178\\n\\n\\t\\t\\t\\t\\n\\ndefunct-datasets/eli5\\n\\n\\nUpdated\\n\\t\\t\\t\\t\\tJan 11, 2024\\n‚Ä¢ \\n\\n\\t\\t\\t\\t\\t20.2k\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t‚Ä¢ \\n\\n\\t\\t\\t\\t\\t52\\n\\n\\t\\t\\t\\t\\n\\nmicrosoft/ms_marco\\n\\n\\n\\t\\t\\tViewer\\n\\t\\t\\t‚Ä¢ \\nUpdated\\n\\t\\t\\t\\t\\tJan 4, 2024\\n‚Ä¢ \\n\\n1.11M\\n‚Ä¢ \\n\\n\\t\\t\\t\\t\\t12.2k\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t‚Ä¢ \\n\\n\\t\\t\\t\\t\\t221\\n\\n\\t\\t\\t\\t\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tSpaces using\\n\\t\\t\\t\\t\\t\\tsentence-transformers/all-MiniLM-L6-v2\\n100\\nüåç\\ncvachet/pdf-chatbot\\nü§ñ\\nPulastya0/Data-Science-Agent\\nüß™\\nWeiZhou-CSU/TRACE-CRN\\nüìö\\u200büîé\\u200büìÑ\\u200b\\npritamdeka/pubmed-abstract-retriever\\nüí¨\\nAixaria/sentence-transformers-all-MiniLM-L6-v2\\nüìö\\nrdisipio/coachable-course-agent\\nüëÅ\\nsiddheshrj/Stranger_things_rag\\nüí¨\\nuniversalsoftware/uchat\\nüöÄ\\ncdhunt96/career_conversation\\nüöÄ\\nJiahongQ/career_conversation\\nüöÄ\\nWilsonT89/Wilson_AI_Career_Asistant\\nüè•\\nkrishna-cc/health_doc\\n\\n+ 95 Spaces\\n\\t\\t\\t\\t\\n+ 88 Spaces\\n\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\tPapers for\\n\\t\\t\\t\\t\\t\\tsentence-transformers/all-MiniLM-L6-v2\\n\\nGooAQ: Open Question Answering with Diverse Answer Types\\n\\n\\t\\t\\tPaper\\n\\t\\t\\t‚Ä¢\\n\\t\\t\\t2104.08727\\n\\t\\t\\t‚Ä¢\\nPublished\\n\\t\\t\\t\\tApr 18, 2021\\n‚Ä¢\\n\\n\\n\\t\\t\\t\\t1\\n\\t\\t\\t\\nPAQ: 65 Million Probably-Asked Questions and What You Can Do With Them\\n\\n\\t\\t\\tPaper\\n\\t\\t\\t‚Ä¢\\n\\t\\t\\t2102.07033\\n\\t\\t\\t‚Ä¢\\nPublished\\n\\t\\t\\t\\tFeb 13, 2021\\n\\nA Repository of Conversational Datasets\\n\\n\\t\\t\\tPaper\\n\\t\\t\\t‚Ä¢\\n\\t\\t\\t1904.06472\\n\\t\\t\\t‚Ä¢\\nPublished\\n\\t\\t\\t\\tApr 13, 2019\\n‚Ä¢\\n\\n\\n\\t\\t\\t\\t5\\n\\t\\t\\t\\nWikiHow: A Large Scale Text Summarization Dataset\\n\\n\\t\\t\\tPaper\\n\\t\\t\\t‚Ä¢\\n\\t\\t\\t1810.09305\\n\\t\\t\\t‚Ä¢\\nPublished\\n\\t\\t\\t\\tOct 18, 2018\\n\\nSearchQA: A New Q&A Dataset Augmented with Context from a Search Engine\\n\\n\\t\\t\\tPaper\\n\\t\\t\\t‚Ä¢\\n\\t\\t\\t1704.05179\\n\\t\\t\\t‚Ä¢\\nPublished\\n\\t\\t\\t\\tApr 18, 2017\\n‚Ä¢\\n\\n\\n\\t\\t\\t\\t1\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tSystem theme\\n\\t\\t\\n\\nCompany\\nTOS\\nPrivacy\\nAbout\\nCareers\\n\\nWebsite\\nModels\\nDatasets\\nSpaces\\nPricing\\nDocs\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7127bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f6220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "770e008e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'title': 'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHugging Face\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tModels\\n\\n\\t\\t\\t\\t\\t\\tDatasets\\n\\n\\t\\t\\t\\t\\t\\tSpaces\\n\\n\\n\\n\\t\\t\\tCommunity\\n\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\tDocs\\n\\n\\t\\t\\t\\t\\t\\tEnterprise\\n\\nPricing\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\nLog In\\n\\t\\t\\t\\t\\nSign Up\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\nsentence-transformers\\n\\n/\\nall-MiniLM-L6-v2\\n\\n\\n\\nlike\\n4.38k\\n\\n\\nFollow\\n\\nSentence Transformers\\n3.49k\\n\\n\\n\\n\\nSentence Similarity\\n\\n\\nsentence-transformers\\n\\n\\nPyTorch\\n\\ngoogle-tensorflow\\nTensorFlow\\n\\n\\nRust\\n\\n\\nONNX\\n\\n\\nSafetensors\\n\\n\\nOpenVINO\\n\\n\\nTransformers\\n\\n\\n\\n21 datasets\\n\\n\\n\\nEnglish\\n\\n\\nbert\\n\\n\\nfeature-extraction\\n\\n\\n\\ntext-embeddings-inference\\n\\n\\n\\narxiv:\\n5 papers\\n\\n\\n\\n\\nLicense:\\napache-2.0\\n\\n\\n\\n\\n\\n\\tModel card\\n\\t\\n\\n\\t\\n\\t\\t\\nFiles\\nFiles and versions\\n\\nxet\\n\\n\\n\\n\\tCommunity\\n\\t147\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tDeploy\\n\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tUse this model\\n\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nall-MiniLM-L6-v2\\nUsage (Sentence-Transformers)\\n\\nUsage (HuggingFace Transformers)\\n\\nBackground\\n\\nIntended uses\\n\\nTraining procedure\\nPre-training\\nFine-tuning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tall-MiniLM-L6-v2'),\n",
       " Document(metadata={'source': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'title': 'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='all-MiniLM-L6-v2\\nUsage (Sentence-Transformers)\\n\\nUsage (HuggingFace Transformers)\\n\\nBackground\\n\\nIntended uses\\n\\nTraining procedure\\nPre-training\\nFine-tuning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tall-MiniLM-L6-v2\\n\\t\\n\\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\\n\\n\\n\\n\\n\\n\\t\\tUsage (Sentence-Transformers)\\n\\t\\n\\nUsing this model becomes easy when you have sentence-transformers installed:\\npip install -U sentence-transformers\\n\\nThen you can use the model like this:\\nfrom sentence_transformers import SentenceTransformer\\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\\n\\nmodel = SentenceTransformer(\\'sentence-transformers/all-MiniLM-L6-v2\\')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\\n\\n\\n\\n\\n\\n\\n\\t\\tUsage (HuggingFace Transformers)'),\n",
       " Document(metadata={'source': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'title': 'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content=\"model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\nembeddings = model.encode(sentences)\\nprint(embeddings)\\n\\n\\n\\n\\n\\n\\n\\t\\tUsage (HuggingFace Transformers)\\n\\t\\n\\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\\nfrom transformers import AutoTokenizer, AutoModel\\nimport torch\\nimport torch.nn.functional as F\\n\\n#Mean Pooling - Take attention mask into account for correct averaging\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\"),\n",
       " Document(metadata={'source': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'title': 'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='# Sentences we want sentence embeddings for\\nsentences = [\\'This is an example sentence\\', \\'Each sentence is converted\\']\\n\\n# Load model from HuggingFace Hub\\ntokenizer = AutoTokenizer.from_pretrained(\\'sentence-transformers/all-MiniLM-L6-v2\\')\\nmodel = AutoModel.from_pretrained(\\'sentence-transformers/all-MiniLM-L6-v2\\')\\n\\n# Tokenize sentences\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=\\'pt\\')\\n\\n# Compute token embeddings\\nwith torch.no_grad():\\n    model_output = model(**encoded_input)\\n\\n# Perform pooling\\nsentence_embeddings = mean_pooling(model_output, encoded_input[\\'attention_mask\\'])\\n\\n# Normalize embeddings\\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\\n\\nprint(\"Sentence embeddings:\")\\nprint(sentence_embeddings)\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tBackground'),\n",
       " Document(metadata={'source': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'title': 'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='print(\"Sentence embeddings:\")\\nprint(sentence_embeddings)\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tBackground\\n\\t\\n\\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \\ncontrastive learning objective. We used the pretrained nreimers/MiniLM-L6-H384-uncased model and fine-tuned in on a \\n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\\nWe developed this model during the \\nCommunity week using JAX/Flax for NLP & CV, \\norganized by Hugging Face. We developed this model as part of the project:\\nTrain the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\\n\\n\\n\\n\\n\\n\\t\\tIntended uses'),\n",
       " Document(metadata={'source': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'title': 'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='Intended uses\\n\\t\\n\\nOur model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \\nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\\nBy default, input text longer than 256 word pieces is truncated.\\n\\n\\n\\n\\n\\n\\t\\tTraining procedure\\n\\t\\n\\n\\n\\n\\n\\n\\n\\t\\tPre-training\\n\\t\\n\\nWe use the pretrained nreimers/MiniLM-L6-H384-uncased model. Please refer to the model card for more detailed information about the pre-training procedure.\\n\\n\\n\\n\\n\\n\\t\\tFine-tuning\\n\\t\\n\\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\\nWe then apply the cross entropy loss by comparing with true pairs.\\n\\n\\n\\n\\n\\n\\t\\tHyper parameters'),\n",
       " Document(metadata={'source': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'title': 'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='Hyper parameters\\n\\t\\n\\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\\na 2e-5 learning rate. The full training script is accessible in this current repository: train_script.py.\\n\\n\\n\\n\\n\\n\\t\\tTraining data\\n\\t\\n\\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\\nWe sampled each dataset given a weighted probability which configuration is detailed in the data_config.json file.\\n\\n\\n\\nDataset\\nPaper\\nNumber of training tuples\\n\\n\\nReddit comments (2015-2018)\\npaper\\n726,484,430\\n\\n\\nS2ORC Citation pairs (Abstracts)\\npaper\\n116,288,806\\n\\n\\nWikiAnswers Duplicate question pairs\\npaper\\n77,427,422\\n\\n\\nPAQ (Question, Answer) pairs\\npaper\\n64,371,441\\n\\n\\nS2ORC Citation pairs (Titles)\\npaper\\n52,603,982\\n\\n\\nS2ORC (Title, Abstract)\\npaper\\n41,769,185'),\n",
       " Document(metadata={'source': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'title': 'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='WikiAnswers Duplicate question pairs\\npaper\\n77,427,422\\n\\n\\nPAQ (Question, Answer) pairs\\npaper\\n64,371,441\\n\\n\\nS2ORC Citation pairs (Titles)\\npaper\\n52,603,982\\n\\n\\nS2ORC (Title, Abstract)\\npaper\\n41,769,185\\n\\n\\nStack Exchange (Title, Body) pairs\\n-\\n25,316,456\\n\\n\\nStack Exchange (Title+Body, Answer) pairs\\n-\\n21,396,559\\n\\n\\nStack Exchange (Title, Answer) pairs\\n-\\n21,396,559\\n\\n\\nMS MARCO triplets\\npaper\\n9,144,553\\n\\n\\nGOOAQ: Open Question Answering with Diverse Answer Types\\npaper\\n3,012,496\\n\\n\\nYahoo Answers (Title, Answer)\\npaper\\n1,198,260\\n\\n\\nCode Search\\n-\\n1,151,414\\n\\n\\nCOCO Image captions\\npaper\\n828,395\\n\\n\\nSPECTER citation triplets\\npaper\\n684,100\\n\\n\\nYahoo Answers (Question, Answer)\\npaper\\n681,164\\n\\n\\nYahoo Answers (Title, Question)\\npaper\\n659,896\\n\\n\\nSearchQA\\npaper\\n582,261\\n\\n\\nEli5\\npaper\\n325,475\\n\\n\\nFlickr 30k\\npaper\\n317,695\\n\\n\\nStack Exchange Duplicate questions (titles)\\n\\n304,525\\n\\n\\nAllNLI (SNLI and MultiNLI\\npaper SNLI, paper MultiNLI\\n277,230\\n\\n\\nStack Exchange Duplicate questions (bodies)\\n\\n250,519'),\n",
       " Document(metadata={'source': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'title': 'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='Flickr 30k\\npaper\\n317,695\\n\\n\\nStack Exchange Duplicate questions (titles)\\n\\n304,525\\n\\n\\nAllNLI (SNLI and MultiNLI\\npaper SNLI, paper MultiNLI\\n277,230\\n\\n\\nStack Exchange Duplicate questions (bodies)\\n\\n250,519\\n\\n\\nStack Exchange Duplicate questions (titles+bodies)\\n\\n250,460\\n\\n\\nSentence Compression\\npaper\\n180,000\\n\\n\\nWikihow\\npaper\\n128,542\\n\\n\\nAltlex\\npaper\\n112,696\\n\\n\\nQuora Question Triplets\\n-\\n103,663\\n\\n\\nSimple Wikipedia\\npaper\\n102,225\\n\\n\\nNatural Questions (NQ)\\npaper\\n100,231\\n\\n\\nSQuAD2.0\\npaper\\n87,599\\n\\n\\nTriviaQA\\n-\\n73,346\\n\\n\\nTotal\\n\\n1,170,060,424\\n\\n\\n\\n\\n\\n\\nDownloads last month149,792,991\\n\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSafetensors\\nModel size\\n22.7M params\\nTensor type\\nI64\\n\\t\\t\\t\\t¬∑F32\\n\\t\\t\\t\\t¬∑\\n\\nFiles info\\n\\n\\n\\n\\n\\n\\n\\nInference Providers\\nNEW\\n\\t\\n\\n\\n\\n\\nHF Inference API\\n\\n\\n\\n\\n\\nSentence Similarity\\n\\n\\n\\nExamples\\n\\n\\n\\n\\n\\nSource Sentence\\n\\n\\n\\nSentences to compare to\\n\\n\\n\\n\\n\\nAdd Sentence\\nGenerate\\n\\n\\n\\n\\nView Code Snippets\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tMaximize\\n\\n\\n\\n\\n\\n\\n\\t\\tModel tree for sentence-transformers/all-MiniLM-L6-v2\\n\\n\\n\\n\\nAdapters\\n\\n14 models\\n\\n\\nFinetunes\\n\\n718 models\\n\\n\\nQuantizations'),\n",
       " Document(metadata={'source': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'title': 'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='Add Sentence\\nGenerate\\n\\n\\n\\n\\nView Code Snippets\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tMaximize\\n\\n\\n\\n\\n\\n\\n\\t\\tModel tree for sentence-transformers/all-MiniLM-L6-v2\\n\\n\\n\\n\\nAdapters\\n\\n14 models\\n\\n\\nFinetunes\\n\\n718 models\\n\\n\\nQuantizations\\n\\n58 models\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tDatasets used to train\\n\\t\\t\\t\\t\\t\\tsentence-transformers/all-MiniLM-L6-v2\\n\\nmandarjoshi/trivia_qa\\n\\n\\n\\t\\t\\tViewer\\n\\t\\t\\t‚Ä¢ \\nUpdated\\n\\t\\t\\t\\t\\tJan 5, 2024\\n‚Ä¢ \\n\\n848k\\n‚Ä¢ \\n\\n\\t\\t\\t\\t\\t43k\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t‚Ä¢ \\n\\n\\t\\t\\t\\t\\t178\\n\\n\\t\\t\\t\\t\\n\\ndefunct-datasets/eli5\\n\\n\\nUpdated\\n\\t\\t\\t\\t\\tJan 11, 2024\\n‚Ä¢ \\n\\n\\t\\t\\t\\t\\t20.2k\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t‚Ä¢ \\n\\n\\t\\t\\t\\t\\t52\\n\\n\\t\\t\\t\\t\\n\\nmicrosoft/ms_marco\\n\\n\\n\\t\\t\\tViewer\\n\\t\\t\\t‚Ä¢ \\nUpdated\\n\\t\\t\\t\\t\\tJan 4, 2024\\n‚Ä¢ \\n\\n1.11M\\n‚Ä¢ \\n\\n\\t\\t\\t\\t\\t12.2k\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t‚Ä¢ \\n\\n\\t\\t\\t\\t\\t221'),\n",
       " Document(metadata={'source': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'title': 'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='Updated\\n\\t\\t\\t\\t\\tJan 11, 2024\\n‚Ä¢ \\n\\n\\t\\t\\t\\t\\t20.2k\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t‚Ä¢ \\n\\n\\t\\t\\t\\t\\t52\\n\\n\\t\\t\\t\\t\\n\\nmicrosoft/ms_marco\\n\\n\\n\\t\\t\\tViewer\\n\\t\\t\\t‚Ä¢ \\nUpdated\\n\\t\\t\\t\\t\\tJan 4, 2024\\n‚Ä¢ \\n\\n1.11M\\n‚Ä¢ \\n\\n\\t\\t\\t\\t\\t12.2k\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t‚Ä¢ \\n\\n\\t\\t\\t\\t\\t221\\n\\n\\t\\t\\t\\t\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tSpaces using\\n\\t\\t\\t\\t\\t\\tsentence-transformers/all-MiniLM-L6-v2\\n100\\nüåç\\ncvachet/pdf-chatbot\\nü§ñ\\nPulastya0/Data-Science-Agent\\nüß™\\nWeiZhou-CSU/TRACE-CRN\\nüìö\\u200büîé\\u200büìÑ\\u200b\\npritamdeka/pubmed-abstract-retriever\\nüí¨\\nAixaria/sentence-transformers-all-MiniLM-L6-v2\\nüìö\\nrdisipio/coachable-course-agent\\nüëÅ\\nsiddheshrj/Stranger_things_rag\\nüí¨\\nuniversalsoftware/uchat\\nüöÄ\\ncdhunt96/career_conversation\\nüöÄ\\nJiahongQ/career_conversation\\nüöÄ\\nWilsonT89/Wilson_AI_Career_Asistant\\nüè•\\nkrishna-cc/health_doc\\n\\n+ 95 Spaces\\n\\t\\t\\t\\t\\n+ 88 Spaces\\n\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\tPapers for\\n\\t\\t\\t\\t\\t\\tsentence-transformers/all-MiniLM-L6-v2\\n\\nGooAQ: Open Question Answering with Diverse Answer Types\\n\\n\\t\\t\\tPaper\\n\\t\\t\\t‚Ä¢\\n\\t\\t\\t2104.08727\\n\\t\\t\\t‚Ä¢\\nPublished\\n\\t\\t\\t\\tApr 18, 2021\\n‚Ä¢\\n\\n\\n\\t\\t\\t\\t1\\n\\t\\t\\t\\nPAQ: 65 Million Probably-Asked Questions and What You Can Do With Them'),\n",
       " Document(metadata={'source': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'title': 'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='Paper\\n\\t\\t\\t‚Ä¢\\n\\t\\t\\t2104.08727\\n\\t\\t\\t‚Ä¢\\nPublished\\n\\t\\t\\t\\tApr 18, 2021\\n‚Ä¢\\n\\n\\n\\t\\t\\t\\t1\\n\\t\\t\\t\\nPAQ: 65 Million Probably-Asked Questions and What You Can Do With Them\\n\\n\\t\\t\\tPaper\\n\\t\\t\\t‚Ä¢\\n\\t\\t\\t2102.07033\\n\\t\\t\\t‚Ä¢\\nPublished\\n\\t\\t\\t\\tFeb 13, 2021\\n\\nA Repository of Conversational Datasets\\n\\n\\t\\t\\tPaper\\n\\t\\t\\t‚Ä¢\\n\\t\\t\\t1904.06472\\n\\t\\t\\t‚Ä¢\\nPublished\\n\\t\\t\\t\\tApr 13, 2019\\n‚Ä¢\\n\\n\\n\\t\\t\\t\\t5\\n\\t\\t\\t\\nWikiHow: A Large Scale Text Summarization Dataset\\n\\n\\t\\t\\tPaper\\n\\t\\t\\t‚Ä¢\\n\\t\\t\\t1810.09305\\n\\t\\t\\t‚Ä¢\\nPublished\\n\\t\\t\\t\\tOct 18, 2018\\n\\nSearchQA: A New Q&A Dataset Augmented with Context from a Search Engine\\n\\n\\t\\t\\tPaper\\n\\t\\t\\t‚Ä¢\\n\\t\\t\\t1704.05179\\n\\t\\t\\t‚Ä¢\\nPublished\\n\\t\\t\\t\\tApr 18, 2017\\n‚Ä¢\\n\\n\\n\\t\\t\\t\\t1\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tSystem theme\\n\\t\\t\\n\\nCompany\\nTOS\\nPrivacy\\nAbout\\nCareers\\n\\nWebsite\\nModels\\nDatasets\\nSpaces\\nPricing\\nDocs')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0b01d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7240764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab166a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstoredb=FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8da077ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1b74f076710>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f3c3837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHugging Face\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tModels\\n\\n\\t\\t\\t\\t\\t\\tDatasets\\n\\n\\t\\t\\t\\t\\t\\tSpaces\\n\\n\\n\\n\\t\\t\\tCommunity\\n\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\tDocs\\n\\n\\t\\t\\t\\t\\t\\tEnterprise\\n\\nPricing\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\nLog In\\n\\t\\t\\t\\t\\nSign Up\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\nsentence-transformers\\n\\n/\\nall-MiniLM-L6-v2\\n\\n\\n\\nlike\\n4.38k\\n\\n\\nFollow\\n\\nSentence Transformers\\n3.49k\\n\\n\\n\\n\\nSentence Similarity\\n\\n\\nsentence-transformers\\n\\n\\nPyTorch\\n\\ngoogle-tensorflow\\nTensorFlow\\n\\n\\nRust\\n\\n\\nONNX\\n\\n\\nSafetensors\\n\\n\\nOpenVINO\\n\\n\\nTransformers\\n\\n\\n\\n21 datasets\\n\\n\\n\\nEnglish\\n\\n\\nbert\\n\\n\\nfeature-extraction\\n\\n\\n\\ntext-embeddings-inference\\n\\n\\n\\narxiv:\\n5 papers\\n\\n\\n\\n\\nLicense:\\napache-2.0\\n\\n\\n\\n\\n\\n\\tModel card\\n\\t\\n\\n\\t\\n\\t\\t\\nFiles\\nFiles and versions\\n\\nxet\\n\\n\\n\\n\\tCommunity\\n\\t147\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tDeploy\\n\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\tUse this model\\n\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nall-MiniLM-L6-v2\\nUsage (Sentence-Transformers)\\n\\nUsage (HuggingFace Transformers)\\n\\nBackground\\n\\nIntended uses\\n\\nTraining procedure\\nPre-training\\nFine-tuning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tall-MiniLM-L6-v2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"This is a sentence-transformers model\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54cea004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.7\n",
      "c:\\Users\\kumar\\Desktop\\anaco\\envs\\langchain\\lib\\site-packages\\langchain\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)\n",
    "print(langchain.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8c082ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import LLMChain\n",
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52c0bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd923449",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "974d2c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_4604\\3001827991.py:12: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use `RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  document_chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n'), additional_kwargs={})]), llm=ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001B736C2A110>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001B736D75870>, model_name='llama-3.1-8b-instant', temperature=1e-08, model_kwargs={}, groq_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc007c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'LangSmith has two usage limits: total traces and extended',\n",
       " 'context': [Document(metadata={}, page_content=\"LangSmith has two usage limits: total traces and extended traces. These correspond to the two metrics we've been tracking on our usage graph. \")],\n",
       " 'text': \"LangSmith has two usage limits: total traces and extended traces. These correspond to the two metrics we've been tracking on our usage graph.\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"LangSmith has two usage limits: total traces and extended\",\n",
    "    \"context\":[Document(page_content=\"LangSmith has two usage limits: total traces and extended traces. These correspond to the two metrics we've been tracking on our usage graph. \")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7504e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstoredb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ae93ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_4604\\597297153.py:43: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  combine_chain = StuffDocumentsChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Unfortunately, there is no information provided about LangChain in the given context. The context only mentions Langsmith, which is a tool for LangChain apps, but it does not provide any details about LangChain itself.\n",
      "\n",
      "Sources:\n",
      "- Langsmith is a tool for LangChain apps.\n",
      "- Langsmith is a tool for LangChain apps.\n",
      "- Langsmith is a tool for managing LangChain apps.\n",
      "- LangSmith is a tool for managing LangChain apps.\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import LLMChain\n",
    "from langchain_classic.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# -------------------------------\n",
    "# 1Ô∏è‚É£ Create LLM\n",
    "# -------------------------------\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 2Ô∏è‚É£ Prompt\n",
    "# -------------------------------\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3Ô∏è‚É£ LLMChain\n",
    "# -------------------------------\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4Ô∏è‚É£ CombineDocumentsChain (FIX)\n",
    "# -------------------------------\n",
    "combine_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 5Ô∏è‚É£ Documents\n",
    "# -------------------------------\n",
    "docs = [\n",
    "    Document(page_content=\"LangSmith is a tool for managing LangChain apps.\"),\n",
    "    Document(page_content=\"LangChain allows chaining of LLMs for QA, summarization, and RAG tasks.\")\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 6Ô∏è‚É£ Vectorstore\n",
    "# -------------------------------\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "vectorstoredb = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "retriever = vectorstoredb.as_retriever()\n",
    "\n",
    "# -------------------------------\n",
    "# 7Ô∏è‚É£ RetrievalQA (NOW VALID)\n",
    "# -------------------------------\n",
    "qa_chain = RetrievalQA(\n",
    "    retriever=retriever,\n",
    "    combine_documents_chain=combine_chain,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 8Ô∏è‚É£ Query\n",
    "# -------------------------------\n",
    "result = qa_chain.invoke({\"query\": \"Can you tell me about LangChain?\"})\n",
    "\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6bd5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
